{
 "cells": [
  {
   "attachments": {
    "%E5%9C%96%E7%89%87.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 陸柏宏\n",
    "\n",
    "Student ID: 109164501\n",
    "\n",
    "GitHub ID: LeafLu0315\n",
    "\n",
    "Kaggle name: LeafLu\n",
    "\n",
    "Kaggle private scoreboard snapshot:![%E5%9C%96%E7%89%87.png](attachment:%E5%9C%96%E7%89%87.png)\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2020-Lab2-Master Repo](https://github.com/fhcalderon87/DM2020-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2020-hw2-nthu/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 5th 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM2020-Lab2-Homework](https://github.com/fhcalderon87/DM2020-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "def countWords(s):\n",
    "    arr = s.split()\n",
    "    dic = dict()\n",
    "    #print(arr)\n",
    "    for word in arr:\n",
    "        if(word in dic):\n",
    "            dic[word] += 1\n",
    "        else:\n",
    "            dic[word] = 1\n",
    "    return dic\n",
    "\n",
    "def list_to_string(arr):\n",
    "    s = \" \"\n",
    "    return s.join(arr)\n",
    "\n",
    "dic_train = countWords(list_to_string(train_df['text']))\n",
    "dic_test = countWords(list_to_string(test_df['text']))\n",
    "\n",
    "dic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top30 = pd.DataFrame(dic_train.items(), columns=['word', 'frequency']).sort_values('frequency', ascending=False).iloc[:30,]\n",
    "train_top30.plot.bar(x = 'word', y = 'frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_top30 = pd.DataFrame(dic_test.items(), columns=['word', 'frequency']).sort_values('frequency', ascending=False).iloc[:30,]\n",
    "test_top30.plot.bar(x = 'word', y = 'frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_f = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_f, tokenizer=nltk.word_tokenize) \n",
    "tfidf.fit(train_df['text'])\n",
    "tfidf_train = tfidf.transform(train_df['text'])\n",
    "tfidf.get_feature_names()[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix calculates how many samples did the computer get wrong or correct.\n",
    "### The y-axis means that the model predicted and the x-axis means what the sample is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred_NB = clf.predict(X_train)\n",
    "y_test_pred_NB = clf.predict(X_test)\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes's acc result is higher than decision tree's\n",
    "### Decision tree's recall&f1-score in anger&sadness are higher than NB's\n",
    "### These can see that the probability for the emotion is better than decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "epoch = training_log.iloc[:,0]\n",
    "acc = training_log.iloc[:,1]\n",
    "loss = training_log.iloc[:,2]\n",
    "val_acc = training_log.iloc[:,3]\n",
    "val_loss = training_log.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc,'b-',label=\"Train accuracy\")\n",
    "plt.plot(val_acc,'r-',label=\"Val accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss,'b-',label=\"Train loss\")\n",
    "plt.plot(val_loss,'r-',label=\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can let the \"word\" counting with a point and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 15\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "sad_words = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \n",
    "fear_words = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('sad_words: ', sad_words)\n",
    "print('fear_words: ', fear_words)\n",
    "\n",
    "target_words = happy_words + angry_words + sad_words + fear_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle ID : LeafLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final result is using BoW and TFIDF vector with different max_features and the part of training and testing set\n",
    "### Below cells are the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here to read data from the json and because of the size of the json is too large, so I use readline then read as json\n",
    "def readData(f):\n",
    "    file = open(f, \"r\")\n",
    "    data = file.readlines()\n",
    "    return data\n",
    "json_file_name = \"tweets_DM.json\"\n",
    "data = readData(json_file_name)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the json text to a list\n",
    "jsonobj = []\n",
    "for i in range(len(data)):\n",
    "    jsonobj.append(json.loads(data[i]))\n",
    "len(jsonobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get familiar with the data\n",
    "jsonobj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonobj[0]['_source']['tweet']['tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonobj[0]['_source']['tweet']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info to an array\n",
    "jsonobj_count = len(jsonobj)\n",
    "arr = []\n",
    "for i in range(jsonobj_count):\n",
    "    arr.append({\"tweet_id\" : jsonobj[i]['_source']['tweet']['tweet_id'],\n",
    "                     \"text\" : jsonobj[i]['_source']['tweet']['text'],\n",
    "                     \"score\" : jsonobj[i]['_score']})\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them to dataFrame\n",
    "df = pd.DataFrame()\n",
    "df = df.append(arr,ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csvs to dataFrame\n",
    "identification_path = \"data_identification.csv\"\n",
    "emotion_path = \"emotion.csv\"\n",
    "identification = pd.read_csv(identification_path)\n",
    "emotion = pd.read_csv(emotion_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data in the original dataFrame\n",
    "all_df = pd.merge(df,emotion)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get how many part of original dataset to be training set\n",
    "percentage = 0.9\n",
    "train_df,test_df = np.split(all_df,[int(percentage*len(all_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predict dataFrame\n",
    "predict_id = []\n",
    "for i in range(len(identification)):\n",
    "    if(identification['identification'][i] == 'test'):\n",
    "        predict_id.append(identification['tweet_id'][i])\n",
    "predict_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame(predict_id,columns=['tweet_id'])\n",
    "df_predict['identification'] = 'test'\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.merge(df,df_predict)\n",
    "predict_df['emotion'] = \"\"\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting BoW\n",
    "BOW_vectorizer = CountVectorizer()\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"😂\" in feature_names_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try about TIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_f = 2000\n",
    "TFIDF_1000 = TfidfVectorizer(max_features=max_f, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "TFIDF_1000.fit(train_df['text'])\n",
    "\n",
    "train_data_TFIDF_features_1000 = TFIDF_1000.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_TFIDF_features_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import nltk\n",
    "max_f = 4000\n",
    "BOW_500 = CountVectorizer(max_features=max_f, tokenizer=nltk.word_tokenize) \n",
    "BOW_500.fit(train_df['text'])\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "# X_train = BOW_500.transform(train_df['text'])\n",
    "X_train = TFIDF_1000.transform(train_df['text'])\n",
    "y_train = train_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = BOW_500.transform(test_df['text'])\n",
    "X_test = TFIDF_1000.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_train = BOW_500.transform(predict_df['text'])\n",
    "predict_train = TFIDF_1000.transform(predict_df['text'])\n",
    "predict_train_res = DT_model.predict(predict_train)\n",
    "predict_train_res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['tweet_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to csv\n",
    "import csv\n",
    "with open('test.csv',\"w\",newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id','emotion'])\n",
    "    for i in range(len(predict_train_res)):\n",
    "        writer.writerow([predict_df['tweet_id'][i],predict_train_res[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
